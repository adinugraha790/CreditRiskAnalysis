# -*- coding: utf-8 -*-
"""Credit RIsk Assessment - v1.0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oAeiCwQVLKDU3hT4SNk-t1Gk9NSF2Rr1
"""

!wget -O data.csv "https://rakamin-lms.s3.ap-southeast-1.amazonaws.com/vix-assets/idx-partners/loan_data_2007_2014.csv"

!pip install catboost

# Basic Libraries
import pandas as pd
import numpy as np
from datetime import datetime

# Visualization
import seaborn as sns
import matplotlib.pyplot as plt

# Preprocessing
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, cross_validate
from sklearn.preprocessing import RobustScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
from sklearn.decomposition import PCA

# Hyperparameter Tuning
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV

# Statistical Tests
from scipy.stats import chi2_contingency, f_oneway, ttest_ind, uniform, randint
from sklearn.feature_selection import mutual_info_classif

# Models
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

# Metrics
from sklearn.metrics import recall_score, accuracy_score, precision_score, roc_auc_score, f1_score, make_scorer, auc
from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay, roc_curve, confusion_matrix, classification_report, ConfusionMatrixDisplay, RocCurveDisplay

# Warnings
import warnings
warnings.filterwarnings("ignore")

# Set display options to show all columns
pd.set_option("display.max_columns", None)  # No column limit
pd.set_option("display.expand_frame_repr", False)  # Prevent wrapping to new lines

# Read the file
df = pd.read_csv("data.csv")
print(df.head())

df.shape

pd.set_option("display.max_rows", None)

pd.Series(df.columns)

df.info()

df = df.drop(columns=["Unnamed: 0"])

df.duplicated().sum()

# Drop columns with 0 non-null values
df = df.dropna(axis=1, how='all')

# Display the resulting DataFrame
print(df.info())

# Convert 'id' and 'member_id' to object (text)
df["id"] = df["id"].astype("object")
df["member_id"] = df["member_id"].astype("object")

df.describe(include=np.number).T

df.describe(include='object').T

# Format data ini tidak teragregasi, levelnya adalah level transaksi atau member. Karena keduanya unik, maka satu orang hanya memiliki satu pinjaman.

# X adalah semua kolom kecuali loan_status, y adalah loan_status

df['loan_status'].value_counts()

# Dataset cukup untuk melakukan pemodelan

"""# Data Cleaning"""

df.info()

print('Missing values status:', df.isnull().values.any())
missing_percentage = pd.DataFrame(df.isnull().sum().sort_values(), columns=['Total Null Values'])
missing_percentage['Percentage'] = (missing_percentage['Total Null Values']/df.shape[0])*100
missing_percentage["Data Type"] = [df[col].dtype for col in df.columns]
missing_percentage.sort_values(by=["Total Null Values", "Percentage"], ascending=False, inplace=True)
missing_percentage.style.background_gradient(cmap='Blues')

# Deleting ............... columns

df = df.drop(['desc',	'mths_since_last_record',	'mths_since_last_major_derog'], axis=1)

# Deleting NA values where the percentage is less than 0.5%
df = df.dropna(subset=['last_pymnt_d', 'revol_util', 'collections_12_mths_ex_med', 'last_credit_pull_d', 'total_acc', 'acc_now_delinq', 'inq_last_6mths', 'earliest_cr_line', 'delinq_2yrs', 'open_acc', 'pub_rec', 'title', 'annual_inc'])

df.info()

df.duplicated().value_counts()

df[df.duplicated(keep=False)].sort_values(by=list(df.columns.values)).head()

"""# Feature Transformation"""

df.describe(include=['object']).T

# Columns to exclude
exclude_columns = ['id', 'member_id', 'emp_title',  'url', 'title', 'zip_code', 'earliest_cr_line',  'last_pymnt_d', 'next_pymnt_d', 'last_credit_pull_d', 'issue_d']

# Select all columns except the excluded ones
selected_columns = df.loc[:, ~df.columns.isin(exclude_columns)]

cat_cols = selected_columns.select_dtypes(include=['object']).columns.tolist()

# Create an empty dictionary to store value counts and percentages for each categorical column
categorical_value_counts = {}

# Iterate through categorical columns and store value counts and percentages in the dictionary
for column in cat_cols:
    # Calculate value counts
    value_counts = selected_columns[column].value_counts()

    # Calculate percentages
    percentages = (value_counts / len(selected_columns)) * 100

    # Combine value counts and percentages into a DataFrame
    categorical_value_counts[column] = pd.DataFrame({
        "Value Counts": value_counts,
        "Percentage": percentages
    })

# Display the value counts and percentages for each categorical column
for column, value_counts_df in categorical_value_counts.items():
    display(value_counts_df.style.set_caption(f"Value Counts and Percentages for {column}"))

df['zip_code'].value_counts()/len(df)*100

# Convert columns to datetime format assuming the 1st of the month
df['issue_d'] = pd.to_datetime(df['issue_d'], format='%b-%y')
df['last_pymnt_d'] = pd.to_datetime(df['last_pymnt_d'], format='%b-%y')
df['next_pymnt_d'] = pd.to_datetime(df['next_pymnt_d'], format='%b-%y')
df['earliest_cr_line'] = pd.to_datetime(df['earliest_cr_line'], format='%b-%y')
df['last_credit_pull_d'] = pd.to_datetime(df['last_credit_pull_d'], format='%b-%y')

df['emp_length'] = df['emp_length'].replace({
    '10+ years': 10,
    '9 years': 9,
    '8 years': 8,
    '7 years': 7,
    '6 years': 6,
    '5 years': 5,
    '4 years': 4,
    '3 years': 3,
    '2 years': 2,
    '1 year': 1,
    "< 1 year": 0
})

df['home_ownership'] = df['home_ownership'].replace({
    'OTHER': 'OTHER',
    'NONE': 'OTHER',
    'ANY': 'OTHER',
    'OWN': 'OWN',
    'MORTGAGE': 'MORTGAGE',
    'RENT': 'RENT'
})

df['policy_code'].value_counts()

df['loan_status'] = df['loan_status'].replace({
    'Fully Paid': 1,
    'Charged Off': 0,
    'Current': 1,
    'In Grace Period': 1,
    'Late (31-120 days)': 0,
    'Late (16-30 days)': 0,
    'Default': 0})

df = df[df['loan_status'].isin([0, 1])]

df['purpose'] = df['purpose'].replace({
    'wedding': 'other',
    'medical': 'other',
    'moving': 'other',
    'vacation': 'other',
    'house': 'other',
    'renewable_energy': 'other',
    'educational': 'other'})

df['purpose'].value_counts()

df['sub_grade'] = df['sub_grade'].replace({
    'F1': 'High Risk',
    'F2': 'High Risk',
    'F3': 'High Risk',
    'F4': 'High Risk',
    'F5': 'High Risk',
    'G1': 'High Risk',
    'G2': 'High Risk',
    'G3': 'High Risk',
    'G4': 'High Risk',
    'G5': 'High Risk',
})

df['addr_state'] = df['addr_state'].replace({
    'KY': 'other',
    'KS': 'other',
    'OK': 'other',
    'AR': 'other',
    'UT': 'other',
    'NM': 'other',
    'HI': 'other',
    'WV': 'other',
    'NH': 'other',
    'RI': 'other',
    'DC': 'other',
    'MT': 'other',
    'DE': 'other',
    'AK': 'other',
    'MS': 'other',
    'WY': 'other',
    'SD': 'other',
    'VT': 'other',
    'IA': 'other',
    'ID': 'other',
    'NE': 'other',
    'ME': 'other'})

df['installment'].value_counts().shape

df.info()

numerical_features = df.select_dtypes(include=['number'])  # Select numerical columns
nunique_counts = numerical_features.nunique()  # Calculate unique value counts

# Display the results
print(nunique_counts)

num_cols = ['inq_last_6mths', 'collections_12_mths_ex_med', 'acc_now_delinq', 'delinq_2yrs', 'pub_rec']

# Create an empty dictionary to store value counts and percentages for each categorical column
numerical_value_counts = {}

# Iterate through categorical columns and store value counts and percentages in the dictionary
for column in num_cols:
    # Calculate value counts
    value_counts = df[column].value_counts()

    # Calculate percentages
    percentages = (value_counts / len(df)) * 100

    # Combine value counts and percentages into a DataFrame
    numerical_value_counts[column] = pd.DataFrame({
        "Value Counts": value_counts,
        "Percentage": percentages
    })

# Display the value counts and percentages for each categorical column
for column, value_counts_df in numerical_value_counts.items():
    display(value_counts_df.style.set_caption(f"Value Counts and Percentages for {column}"))

df.info()

"""# DROOP BUAT FEATURE ENGINEERING & ALASANNYAAAA

"""

df['issue_yr'] = df['issue_d'].dt.year
df['issue_mon'] = df['issue_d'].dt.month

df['last_pymnt_yr'] = df['last_pymnt_d'].dt.year
df['last_pymnt_mon'] = df['last_pymnt_d'].dt.month

df = df.drop(['next_pymnt_d', 'last_credit_pull_d', 'pymnt_plan', 'application_type', 'zip_code', 'policy_code', 'id', 'member_id', 'emp_title',  'url', 'title'], axis=1)

# Current date for reference (assuming today for calculations)
current_date = datetime.now()

# 1. Loan-to-Income Ratio
df['loan_to_income_ratio'] = df['loan_amnt'] / df['annual_inc']

# 2. Payment Progress
df['payment_progress'] = df['total_pymnt'] / df['funded_amnt']

# 3. Time-Based Features
# Loan Age (in months)
df['issue_d'] = pd.to_datetime(df['issue_d'], errors='coerce')  # Convert to datetime
df['loan_age_months'] = ((current_date - df['issue_d']).dt.days / 30).fillna(0)

# Time Since Last Payment (in months)
df['last_pymnt_d'] = pd.to_datetime(df['last_pymnt_d'], errors='coerce')
df['time_since_last_payment_months'] = ((current_date - df['last_pymnt_d']).dt.days / 30).fillna(np.nan)

# 4. Derived Credit History Length
df['earliest_cr_line'] = pd.to_datetime(df['earliest_cr_line'], errors='coerce')
df['credit_history_length_years'] = ((df['issue_d'] - df['earliest_cr_line']).dt.days / 365).fillna(0)

# 5. High Credit Utilization Flags
df['high_credit_util_flag'] = (df['revol_util'] > 80).astype(int)

df.info()

"""# Exploratory Data Analysis"""

# Identifying the categorical variables
cat_var = df.select_dtypes(include=["object"]).columns

# Plotting bar chart for each categorical variable
plt.style.use("seaborn-v0_8-darkgrid")  # Use a clean style
palette = plt.cm.Paired.colors  # Choose a color palette

for i, column in enumerate(cat_var, start=1):
    # Create figure for each variable
    plt.figure(figsize=(8, 4))  # Set figure size

    # Bar plot
    df[column].value_counts().plot(
        kind="bar",
        color=palette,
        edgecolor="black",
        alpha=0.85,
    )

    # Add labels and title
    plt.xlabel(column, fontsize=14, labelpad=10)
    plt.ylabel("Count", fontsize=14, labelpad=10)
    plt.title(f"Distribution of {column}", fontsize=16, pad=15)

    # Rotate x-axis labels for better readability
    plt.xticks(rotation=45, fontsize=12)
    plt.yticks(fontsize=12)

    # Add gridlines
    plt.grid(axis="y", linestyle="--", alpha=0.7)

    # Show plot
    plt.tight_layout()  # Adjust layout for better spacing
    plt.show()

# Select numerical variables
num_var = df.select_dtypes(include=np.number).columns

# Define the number of rows and columns for subplots
rows, cols = 5, 9
fig, axes = plt.subplots(rows, cols, figsize=(30, 15))  # Adjust figsize for better visualization
fig.tight_layout(pad=5.0)  # Adjust spacing between subplots

# Flatten the axes for easier iteration
axes = axes.flatten()

# Plot each histogram
for idx, column in enumerate(num_var):
    # Compute statistics
    average = df[column].mean()
    median = df[column].median()
    mode = df[column].mode()
    min_val = df[column].min()
    max_val = df[column].max()
    q1 = df[column].quantile(0.25)
    q3 = df[column].quantile(0.75)

    # Plot histogram on the corresponding subplot
    sns.histplot(df[column], kde=True, bins=30, color="skyblue", edgecolor="black", alpha=0.7, ax=axes[idx])

    # Add vertical lines for statistics
    axes[idx].axvline(average, color="red", linestyle="solid", linewidth=2.5, label=f'Mean: {average:.2f}')
    axes[idx].axvline(median, color="orange", linestyle="dotted", linewidth=2.5, label=f'Median: {median:.2f}')
    axes[idx].axvline(mode[0], color="purple", linestyle="dashed", linewidth=2.5, label=f'Mode: {mode[0]:.2f}')
    axes[idx].axvline(min_val, color="blue", linestyle="dashdot", linewidth=2, label=f'Min: {min_val:.2f}')
    axes[idx].axvline(max_val, color="green", linestyle="dashdot", linewidth=2, label=f'Max: {max_val:.2f}')
    axes[idx].axvline(q1, color="cyan", linestyle="dashed", linewidth=1.5, label=f'Q1: {q1:.2f}')
    axes[idx].axvline(q3, color="magenta", linestyle="dashed", linewidth=1.5, label=f'Q3: {q3:.2f}')

    # Add title and legend to each subplot
    axes[idx].set_title(column, fontsize=10)
    axes[idx].legend(fontsize=8)

# Hide unused subplots if there are any
for i in range(len(num_var), len(axes)):
    fig.delaxes(axes[i])

# Show the combined plot
plt.show()

# Categorical columns to visualize
categorical_columns = df.select_dtypes(include=['object']).columns

# Set Seaborn style
sns.set_theme(style="whitegrid")

# Create subplots: 1 column (1 graph per row)
num_plots = len(categorical_columns)
fig, axes = plt.subplots(nrows=num_plots, ncols=1, figsize=(12, 5 * num_plots))
axes = axes.flatten() if num_plots > 1 else [axes]  # Ensure axes is always a list

# Iterate through each categorical column
for i, column in enumerate(categorical_columns):
    ax = axes[i]

    try:
        # Calculate normalized value counts grouped by loan_status
        grouped_data = (
            df.groupby(column)['loan_status']
            .value_counts(normalize=True)
            .unstack('loan_status', fill_value=0)  # Fill missing values with 0
        )

        # Ensure columns are ordered correctly
        if grouped_data.columns.size == 2:  # Only reorder if exactly two categories exist
            grouped_data = grouped_data.sort_index(axis=1)

        # Plot stacked bar charts
        sns.barplot(
            x=grouped_data.index,
            y=grouped_data.iloc[:, 1],  # Second column as 'Low'
            color='lightblue',
            label='Low',
            ax=ax
        )
        sns.barplot(
            x=grouped_data.index,
            y=grouped_data.iloc[:, 0],  # First column as 'High'
            color='lightcoral',
            label='High',
            bottom=grouped_data.iloc[:, 1],  # Stack on top of 'Low'
            ax=ax
        )

        # Set titles and labels
        ax.set_title(f'Percentage High/Low Risk for {column}', fontsize=14, fontweight='bold')
        ax.set_ylabel('Percentage', fontsize=12)
        ax.set_xlabel(column, fontsize=12)
        ax.legend(labels=('High', 'Low'), fontsize=10)

        # Adjust x-axis ticks for readability
        ax.set_xticklabels(ax.get_xticklabels(), rotation=90, fontsize=10)

    except Exception as e:
        print(f"Skipping column '{column}' due to error: {e}")

# Adjust layout for better spacing
plt.tight_layout()
plt.show()

# List of numerical columns to visualize
numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns

# Set Seaborn style
sns.set_theme(style="whitegrid")

# Create subplots: 1 column (1 graph per row)
num_plots = len(numerical_columns)
fig, axes = plt.subplots(nrows=num_plots, ncols=1, figsize=(12, 5 * num_plots))
axes = axes.flatten() if num_plots > 1 else [axes]  # Ensure axes is always a list

# Iterate through each numerical column
for i, column in enumerate(numerical_columns):
    ax = axes[i]

    # Create the violin plot for each numerical column, grouped by 'loan_status'
    sns.violinplot(
        x='loan_status',  # hue or category for splitting
        y=column,  # Numerical variable
        data=df,
        ax=ax,
        palette='Set2',  # Adjust color palette if needed
        split=True,  # Split the violins by 'loan_status'
        inner="quart",  # Show the quartiles inside the violins
    )

    # Set titles and labels
    ax.set_title(f'Distribution of {column} by Loan Status', fontsize=14, fontweight='bold')
    ax.set_xlabel('Loan Status', fontsize=12)
    ax.set_ylabel(column, fontsize=12)

# Adjust layout for better spacing
plt.tight_layout()
plt.show()

df['loan_status'] = df['loan_status'].astype(int)

# Select numeric columns
num_cols = df.select_dtypes(include=['float64', 'int64'])

# Calculate correlation and multiply by 100 to get percentages
correlation_matrix = num_cols.corr() * 100 // 10

# Create the heatmap with single digit percentage
plt.figure(figsize=(14, 8))
sns.heatmap(correlation_matrix, annot=True, fmt='.0f', cmap='coolwarm', cbar_kws={'label': 'Percentage (%)'}, annot_kws={"size": 10})

# Title
plt.title('Correlation Matrix (as 10 times percentage)')

# Show the plot
plt.show()

df_ = df.drop(columns=['funded_amnt', 'funded_amnt_inv', 'out_prncp_inv', 'total_pymnt_inv', 'total_pymnt_inv', 'total_rec_int', 'total_rec_prncp','installment', 'collection_recovery_fee', 'total_rev_hi_lim', 'total_pymnt', 'time_since_last_payment_months'], axis=1)

df['loan_status'] = df['loan_status'].astype(int)

# Select numeric columns
num_cols = df_.select_dtypes(include=['float64', 'int64'])

# Calculate correlation and multiply by 100 to get percentages
correlation_matrix = num_cols.corr() * 100 // 10

# Create the heatmap with single digit percentage
plt.figure(figsize=(14, 8))
sns.heatmap(correlation_matrix, annot=True, fmt='.0f', cmap='coolwarm', cbar_kws={'label': 'Percentage (%)'}, annot_kws={"size": 10})

# Title
plt.title('Correlation Matrix (as 10 times percentage)')

# Show the plot
plt.show()

df = df_

# List of columns to test
columns_to_test = df.select_dtypes(include=['float64', 'int64']).columns.to_list()
columns_to_test = [col for col in columns_to_test if col != 'loan_status']  # Exclude 'loan_status' column

# List to store results
results = []

# Perform t-tests and store results
alpha = 0.05

for column in columns_to_test:
    group1 = df[df['loan_status'] == 1][column].dropna()
    group2 = df[df['loan_status'] == 0][column].dropna()

    t_statistic, p_value = ttest_ind(group1, group2)

    # Store results in the list
    results.append({'Column': column, 'P-Value': p_value})

# Convert the results into a DataFrame
results_df = pd.DataFrame(results)

# Display the results DataFrame
results_df

insignificant_results_df = results_df[results_df['P-Value'] > alpha]
print("\nInsignificant Results:")
print(insignificant_results_df)

# Convert 'loan_status' to string if needed
df['loan_status'] = df['loan_status'].astype(str)

# Select categorical columns
cats = df.select_dtypes(include=['object']).columns.to_list()

# Initialize DataFrames to store chi-square test results
chi2_results = pd.DataFrame(index=cats, columns=cats)  # For Chi-square statistics
p_values = pd.DataFrame(index=cats, columns=cats)      # For p-values

# Handle missing values: Drop rows with NaN in categorical columns
df_clean = df[cats].dropna()

# Perform chi-square test for each pair of categorical variables
for feature1 in cats:
    for feature2 in cats:
        # Create a contingency table
        contingency_table = pd.crosstab(df_clean[feature1], df_clean[feature2])

        # Perform chi-square test
        chi2, p, _, _ = chi2_contingency(contingency_table)

        # Store the results in the DataFrames
        chi2_results.at[feature1, feature2] = chi2
        p_values.at[feature1, feature2] = p

# Convert p-values to numeric type for visualization
p_values = p_values.astype(float)

# Display the chi-square test results
print("Chi-Square Test Results (Chi-Square Statistics):")
print(chi2_results)

# Display the p-values
print("\nP-Values:")
print(p_values)

results_df_cat = np.where(p_values > alpha, 'Insignificant', 'Significant')
results_df_cat = pd.DataFrame(results_df_cat, index=cats, columns=cats)
results_df_cat

"""Kesimpulan

# Modelling
"""

df.info()

unrelated_cols = ['loan_amnt', 'initial_list_status', 'out_prncp', 'total_rec_late_fee', 'recoveries', 'last_pymnt_amnt', 'collections_12_mths_ex_med', 'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal', 'issue_yr', 'issue_mon', 'last_pymnt_yr', 'last_pymnt_mon', 'loan_to_income_ratio', 'payment_progress', 'loan_age_months', 'credit_history_length_years', 'high_credit_util_flag']

df = df.drop(columns=unrelated_cols, axis=1)

dtime_cols = ['issue_d', 'last_pymnt_d', 'earliest_cr_line']

df = df.drop(columns=dtime_cols, axis=1)

df['loan_status'] = df['loan_status'].astype(int)

df['loan_status'].value_counts()

df.info()

X, y = df.drop('loan_status', axis=1), df['loan_status']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

categorical_features = X_train.select_dtypes(include=['object']).columns.to_list()
numerical_features = X_train.select_dtypes(include=['float64', 'int64']).columns.to_list()

# Define the numerical transformer
numerical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),  # Handle missing values
    ("scaler", RobustScaler()) # Standardize numerical features
])

# Define the categorical transformer
categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),  # Handle missing values in categorical columns
    ("onehot", OneHotEncoder(handle_unknown="ignore"))  # One-hot encode categorical features
])

# Define the preprocessor using ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ("num", numerical_transformer, numerical_features),
        ("cat", categorical_transformer, categorical_features)
    ]
)

# Define the model pipeline with increased iterations and scaling
def create_pipeline(model):
    return ImbPipeline([
        ("preprocessor", preprocessor),  # Apply preprocessing steps
        ("smote", SMOTE(random_state=42)),  # Apply SMOTE for balancing
        ("model", model)  # Specify the ML model
    ])

# Evaluate each model using multiple metrics
metrics = {
    'Recall': make_scorer(recall_score),
    'Accuracy': make_scorer(accuracy_score),
    'Precision': make_scorer(precision_score),
    'ROC AUC': make_scorer(roc_auc_score, needs_proba=True),
    'F1 Score': make_scorer(f1_score)
}

# Perform Stratified K-Fold Cross Validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Define models
models = {
    "Logistic Regression": LogisticRegression(solver='saga', max_iter=100, random_state=42)
    #,"Decision Tree": DecisionTreeClassifier(),
    #"Random Forest": RandomForestClassifier(),
    #"XGBoost": XGBClassifier(eval_metric="logloss"),
    #"LightGBM": LGBMClassifier(),
    #"CatBoost": CatBoostClassifier(verbose=0),
    #"SVM": SVC(probability=True),
    #"KNN": KNeighborsClassifier(),
    #"Neural Network": MLPClassifier(max_iter=1000)
}

# Perform Stratified K-Fold Cross Validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

results = {}

for name, model in models.items():
    pipeline = create_pipeline(model)

    # Perform cross-validation with multiple scoring metrics
    scores = cross_validate(pipeline, X_train, y_train, scoring=metrics, cv=cv, return_train_score=False)

    # Store results for each metric
    results[name] = {
        metric_name: {
            "Mean": np.mean(scores[f"test_{metric_name}"]),
            "Std Dev": np.std(scores[f"test_{metric_name}"])
        }
        for metric_name in metrics.keys()
    }

# Convert results to a DataFrame for better visualization
results_df = pd.DataFrame({
    model_name: {
        metric_name: f"Mean = {metric_scores['Mean']:.4f}, Std Dev = {metric_scores['Std Dev']:.4f}"
        for metric_name, metric_scores in metrics_scores.items()
    }
    for model_name, metrics_scores in results.items()
})

print("\nModel Performance (Mean and Std Dev for Each Metric):")
print(results_df)

results_df

# Evaluate the trained model on the test set
test_results = {}

for name, model in models.items():
    # Create and train the pipeline
    pipeline = create_pipeline(model)
    pipeline.fit(X_train, y_train)

    # Predict on the test set
    y_pred = pipeline.predict(X_test)
    y_pred_proba = pipeline.predict_proba(X_test)[:, 1] if hasattr(pipeline, "predict_proba") else None

    # Calculate metrics
    test_results[name] = {
        "Recall": recall_score(y_test, y_pred),
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred),
        "F1 Score": f1_score(y_test, y_pred),
        "ROC AUC": roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else "N/A",
    }

# Convert results to a DataFrame for better visualization
test_results_df = pd.DataFrame(test_results).T

print("\nTest Set Performance:")
print(test_results_df)

model_dt = {
    #"Logistic Regression": LogisticRegression(max_iter=5000, solver='lbfgs')
    "Decision Tree": DecisionTreeClassifier()
    #"Random Forest": RandomForestClassifier(),
    #"XGBoost": XGBClassifier(eval_metric="logloss"),
    #"LightGBM": LGBMClassifier(),
    #"CatBoost": CatBoostClassifier(verbose=0),
    #"SVM": SVC(probability=True),
    #"KNN": KNeighborsClassifier(),
    #"Neural Network": MLPClassifier(max_iter=1000)
}

results = {}

for name, model in model_dt.items():
    pipeline = create_pipeline(model)

    # Perform cross-validation with multiple scoring metrics
    scores = cross_validate(pipeline, X_train, y_train, scoring=metrics, cv=cv, return_train_score=False)

    # Store results for each metric
    results[name] = {
        metric_name: {
            "Mean": np.mean(scores[f"test_{metric_name}"]),
            "Std Dev": np.std(scores[f"test_{metric_name}"])
        }
        for metric_name in metrics.keys()
    }

# Convert results to a DataFrame for better visualization
result_dt_df = pd.DataFrame({
    model_name: {
        metric_name: f"Mean = {metric_scores['Mean']:.4f}, Std Dev = {metric_scores['Std Dev']:.4f}"
        for metric_name, metric_scores in metrics_scores.items()
    }
    for model_name, metrics_scores in results.items()
})

result_dt_df

# Evaluate the trained model on the test set
test_results = {}

for name, model in model_dt.items():
    # Create and train the pipeline
    pipeline = create_pipeline(model)
    pipeline.fit(X_train, y_train)

    # Predict on the test set
    y_pred = pipeline.predict(X_test)
    y_pred_proba = pipeline.predict_proba(X_test)[:, 1] if hasattr(pipeline, "predict_proba") else None

    # Calculate metrics
    test_results[name] = {
        "Recall": recall_score(y_test, y_pred),
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred),
        "F1 Score": f1_score(y_test, y_pred),
        "ROC AUC": roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else "N/A",
    }

# Convert results to a DataFrame for better visualization
test_result_dt_df = pd.DataFrame(test_results).T

print("\nTest Set Performance:")
print(test_result_dt_df)

model_xgb = {
    #"Logistic Regression": LogisticRegression(max_iter=5000, solver='lbfgs')
    #"Decision Tree": DecisionTreeClassifier()
    #"Random Forest": RandomForestClassifier()
    "XGBoost": XGBClassifier(eval_metric="logloss")
    #"LightGBM": LGBMClassifier(),
    #"CatBoost": CatBoostClassifier(verbose=0),
    #"SVM": SVC(probability=True),
    #"KNN": KNeighborsClassifier(),
    #"Neural Network": MLPClassifier(max_iter=1000)
}

results = {}

for name, model in model_xgb.items():
    pipeline = create_pipeline(model)

    # Perform cross-validation with multiple scoring metrics
    scores = cross_validate(pipeline, X_train, y_train, scoring=metrics, cv=cv, return_train_score=False)

    # Store results for each metric
    results[name] = {
        metric_name: {
            "Mean": np.mean(scores[f"test_{metric_name}"]),
            "Std Dev": np.std(scores[f"test_{metric_name}"])
        }
        for metric_name in metrics.keys()
    }

# Convert results to a DataFrame for better visualization
result_xgb_df = pd.DataFrame({
    model_name: {
        metric_name: f"Mean = {metric_scores['Mean']:.4f}, Std Dev = {metric_scores['Std Dev']:.4f}"
        for metric_name, metric_scores in metrics_scores.items()
    }
    for model_name, metrics_scores in results.items()
})

result_xgb_df

# Evaluate the trained model on the test set
test_results = {}

for name, model in model_xgb.items():
    # Create and train the pipeline
    pipeline = create_pipeline(model)
    pipeline.fit(X_train, y_train)

    # Predict on the test set
    y_pred = pipeline.predict(X_test)
    y_pred_proba = pipeline.predict_proba(X_test)[:, 1] if hasattr(pipeline, "predict_proba") else None

    # Calculate metrics
    test_results[name] = {
        "Recall": recall_score(y_test, y_pred),
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred),
        "F1 Score": f1_score(y_test, y_pred),
        "ROC AUC": roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else "N/A",
    }

# Convert results to a DataFrame for better visualization
test_result_xgb_df = pd.DataFrame(test_results).T

print("\nTest Set Performance:")
print(test_result_xgb_df)

# Define a streamlined hyperparameter grid
xgb_param_grid = {
    'model__n_estimators': [50, 100],  # Focus on smaller ranges
    'model__learning_rate': [0.05, 0.1],  # Conservative step sizes
    'model__max_depth': [3, 5],  # Fewer options for tree depth
    'model__subsample': [0.8, 1.0],  # Slight variation in subsample ratio
    'model__colsample_bytree': [0.8, 1.0],  # Column sampling ratios
    'model__gamma': [0, 1],  # Simpler gamma range
}

# Set up RandomizedSearchCV
random_search_xgb = RandomizedSearchCV(
    estimator=pipeline,
    param_distributions=xgb_param_grid,
    n_iter=8,  # Fewer combinations for faster execution
    scoring='roc_auc',  # Optimize for ROC AUC
    cv=cv,  # Stratified K-Fold Cross-Validation
    verbose=1,
    random_state=42,
    n_jobs=-1  # Use all available CPU cores
)

# Fit the RandomizedSearchCV to the training data
random_search_xgb.fit(X_train, y_train)

# Print results
print("Best Parameters for XGBoost:", random_search_xgb.best_params_)
print("Best ROC AUC Score (Train):", random_search_xgb.best_score_)

# Evaluate on the test set
best_xgb_pipeline = random_search_xgb.best_estimator_
y_test_pred_proba = best_xgb_pipeline.predict_proba(X_test)[:, 1]
roc_auc_test = roc_auc_score(y_test, y_test_pred_proba)
print("Test ROC AUC Score:", roc_auc_test)

# Evaluate on the test set
best_xgb_pipeline = random_search_xgb.best_estimator_
y_test_pred_proba = best_xgb_pipeline.predict_proba(X_test)[:, 1]
y_test_pred = best_xgb_pipeline.predict(X_test)
roc_auc_test = roc_auc_score(y_test, y_test_pred_proba)
print("Test ROC AUC Score:", roc_auc_test)

# Calculate metrics for the test set
test_metrics = {
    "Recall": recall_score(y_test, y_test_pred),
    "Accuracy": accuracy_score(y_test, y_test_pred),
    "Precision": precision_score(y_test, y_test_pred),
    "F1 Score": f1_score(y_test, y_test_pred),
    "ROC AUC": roc_auc_score(y_test, y_test_pred_proba),
}

# Display the test set metrics
print("\nTest Set Performance with Tuned Model:")
print(pd.DataFrame(test_metrics, index=["Score"]).T)

# Evaluate on the test set
best_xgb_pipeline = random_search_xgb.best_estimator_
y_train_pred_proba = best_xgb_pipeline.predict_proba(X_train)[:, 1]
y_train_pred = best_xgb_pipeline.predict(X_train)
roc_auc_test = roc_auc_score(y_train, y_train_pred_proba)
print("Test ROC AUC Score:", roc_auc_test)

# Calculate metrics for the test set
test_metrics = {
    "Recall": recall_score(y_train, y_train_pred),
    "Accuracy": accuracy_score(y_train, y_train_pred),
    "Precision": precision_score(y_train, y_train_pred),
    "F1 Score": f1_score(y_train, y_train_pred),
    "ROC AUC": roc_auc_score(y_train, y_train_pred_proba),
}

# Display the test set metrics
print("\nTrain Set Performance with Tuned Model:")
print(pd.DataFrame(test_metrics, index=["Score"]).T)

# Evaluate the tuned model on the training set
y_train_pred = best_xgb_pipeline.predict(X_train)  # Predict the class labels for training set
y_train_pred_proba = best_xgb_pipeline.predict_proba(X_train)[:, 1]  # Predict probabilities for training set

# Calculate training metrics
train_recall = recall_score(y_train, y_train_pred)
train_accuracy = accuracy_score(y_train, y_train_pred)
train_precision = precision_score(y_train, y_train_pred)
train_f1 = f1_score(y_train, y_train_pred)
roc_auc_train = roc_auc_score(y_train, y_train_pred_proba)

# Evaluate the tuned model on the test set
y_test_pred = best_xgb_pipeline.predict(X_test)  # Predict the class labels for test set
y_test_pred_proba = best_xgb_pipeline.predict_proba(X_test)[:, 1]  # Predict probabilities for test set

# Calculate test metrics
test_recall = recall_score(y_test, y_test_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_precision = precision_score(y_test, y_test_pred)
test_f1 = f1_score(y_test, y_test_pred)
roc_auc_test = roc_auc_score(y_test, y_test_pred_proba)

# Print the results
print("\nTraining Set Metrics for Tuned XGBoost:")
print(f"Recall: {train_recall:.4f}")
print(f"Accuracy: {train_accuracy:.4f}")
print(f"Precision: {train_precision:.4f}")
print(f"F1 Score: {train_f1:.4f}")
print(f"ROC AUC: {roc_auc_train:.4f}")

print("\nTest Set Metrics for Tuned XGBoost:")
print(f"Recall: {test_recall:.4f}")
print(f"Accuracy: {test_accuracy:.4f}")
print(f"Precision: {test_precision:.4f}")
print(f"F1 Score: {test_f1:.4f}")
print(f"ROC AUC: {roc_auc_test:.4f}")

# Optionally, display classification reports for further insights
print("\nClassification Report for Training Set:")
print(classification_report(y_train, y_train_pred))

print("\nClassification Report for Test Set:")
print(classification_report(y_test, y_test_pred))

# Generate predicted probabilities for the positive class (1)
y_pred_curve = pipeline.predict_proba(X_test)[:, 1]  # Use the best pipeline from tuning

# Compute precision, recall, and thresholds
precision, recall, threshold = precision_recall_curve(y_test, y_pred_curve)

# Plot the Precision-Recall curve
prd = PrecisionRecallDisplay(precision, recall)
prd.plot()
plt.title("Precision-Recall Curve for XGBoost Model")
plt.show()

# Generate predicted probabilities for the positive class (1)
y_pred_curve = pipeline.predict_proba(X_test)[:, 1]  # Use the best pipeline from tuning

# Compute precision, recall, and thresholds
precision, recall, threshold = precision_recall_curve(y_test, y_pred_curve)

pr_table = pd.DataFrame({
    "Threshold": threshold,
    "Precision": precision[:-1],  # Last value of precision is omitted
    "Recall": recall[:-1],        # Last value of recall is omitted
})

# Find the threshold where precision is close to recall
pr_table["Difference"] = abs(pr_table["Precision"] - pr_table["Recall"])
optimal_threshold = pr_table.loc[pr_table["Difference"].idxmin(), "Threshold"]
print("Optimal Threshold:", optimal_threshold)
print("Optimal Recall:", pr_table.loc[pr_table["Difference"].idxmin(), ["Recall"]])
print("Optimal Precision:", pr_table.loc[pr_table["Difference"].idxmin(), ["Precision"]])

plt.figure(figsize=(10, 6))
plt.plot(threshold, precision[:-1], label="Precision", color="blue")
plt.plot(threshold, recall[:-1], label="Recall", color="orange")
plt.xlabel("Threshold", fontsize=14)
plt.ylabel("Score", fontsize=14)
plt.title("Precision-Recall vs. Threshold", fontsize=16)
plt.legend(loc="lower left", fontsize=12)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 6))
plt.plot(threshold, precision[:-1], label="Precision", color="blue")
plt.plot(threshold, recall[:-1], label="Recall", color="orange")
plt.xlabel("Threshold", fontsize=14)
plt.ylabel("Score", fontsize=14)
plt.title("Precision-Recall vs. Threshold", fontsize=16)
plt.legend(loc="lower left", fontsize=12)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# Train the pipeline if not already fitted
pipeline.fit(X_train, y_train)

# Extract the trained model from the pipeline
model = pipeline.named_steps['model']

# Ensure the model provides feature importances (tree-based models like LightGBM)
if hasattr(model, 'feature_importances_'):
    # Extract feature importances
    feature_importances = model.feature_importances_

    # Get the numerical feature names
    numerical_features = preprocessor.transformers_[0][2]

    # Get the categorical feature names from the one-hot encoder
    categorical_features = pipeline.named_steps['preprocessor'].named_transformers_['cat']['onehot'].get_feature_names_out()

    # Combine numerical and categorical feature names
    all_features = list(numerical_features) + list(categorical_features)

    # Ensure the lengths match between feature names and importances
    if len(feature_importances) != len(all_features):
        print("Length mismatch between feature importances and feature names.")
    else:
        # Create a DataFrame for feature importance
        feature_importance_df = pd.DataFrame({'Feature': all_features, 'Importance': feature_importances})

        # Sort by importance for better visualization
        feature_importance_df = feature_importance_df.sort_values('Importance', ascending=True)

        # Plot feature importance
        plt.figure(figsize=(10, 20))
        feature_importance_df.plot(x='Feature', y='Importance', kind='barh', figsize=(10, 20), legend=False, color='skyblue')
        plt.title('Feature Importance')
        plt.xlabel('Importance')
        plt.ylabel('Feature')
        plt.tight_layout()
        plt.show()

else:
    print("The selected model does not provide feature importances.")

# Assuming the pipeline has been fitted on the training data
y_pred = pipeline.predict(X_test)  # Predict on the test set

# Generate confusion matrix
cm_xgb = confusion_matrix(y_test, y_pred)

# Plot confusion matrix using seaborn heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

TP = cm_xgb[1][1]
FN = cm_xgb[1][0]
FP = cm_xgb[0][1]
TN = cm_xgb[0][0]

persentase_kesalahan_model = (FP / (TP+FP+TN+FN)) * 100

print(f'Persentase kesalahan prediksi resiko pinjaman yang menimbulkan kerugian: {persentase_kesalahan_model:.2f}%')

persentase_kesalahan_model = (FN / (TP+FP+TN+FN)) * 100

print(f'Persentase kesalahan prediksi resiko pinjaman yang menimbulkan profit tidak maksimal: {persentase_kesalahan_model:.2f}%')

model_lgbm = {
    #"Logistic Regression": LogisticRegression(max_iter=5000, solver='lbfgs')
    #"Decision Tree": DecisionTreeClassifier()
    #"Random Forest": RandomForestClassifier()
    #"XGBoost": XGBClassifier(eval_metric="logloss")
    "LightGBM": LGBMClassifier()
    #"CatBoost": CatBoostClassifier(verbose=0),
    #"SVM": SVC(probability=True)
    #"KNN": KNeighborsClassifier()
    #"Neural Network": MLPClassifier(max_iter=1000)
}

results = {}

for name, model in model_lgbm.items():
    pipeline = create_pipeline(model)

    # Perform cross-validation with multiple scoring metrics
    scores = cross_validate(pipeline, X_train, y_train, scoring=metrics, cv=cv, return_train_score=False)

    # Store results for each metric
    results[name] = {
        metric_name: {
            "Mean": np.mean(scores[f"test_{metric_name}"]),
            "Std Dev": np.std(scores[f"test_{metric_name}"])
        }
        for metric_name in metrics.keys()
    }

# Convert results to a DataFrame for better visualization
result_lgbm_df = pd.DataFrame({
    model_name: {
        metric_name: f"Mean = {metric_scores['Mean']:.4f}, Std Dev = {metric_scores['Std Dev']:.4f}"
        for metric_name, metric_scores in metrics_scores.items()
    }
    for model_name, metrics_scores in results.items()
})

result_lgbm_df

# Evaluate the trained model on the test set
test_results = {}

for name, model in model_lgbm.items():
    # Create and train the pipeline
    pipeline = create_pipeline(model)
    pipeline.fit(X_train, y_train)

    # Predict on the test set
    y_pred = pipeline.predict(X_test)
    y_pred_proba = pipeline.predict_proba(X_test)[:, 1] if hasattr(pipeline, "predict_proba") else None

    # Calculate metrics
    test_results[name] = {
        "Recall": recall_score(y_test, y_pred),
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred),
        "F1 Score": f1_score(y_test, y_pred),
        "ROC AUC": roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else "N/A",
    }

# Convert results to a DataFrame for better visualization
test_result_lgbm_df = pd.DataFrame(test_results).T

print("\nTest Set Performance:")
print(test_result_lgbm_df)

# Define a smaller parameter grid
param_dist = {
    "model__num_leaves": randint(20, 40),  # Narrow range for number of leaves
    "model__learning_rate": uniform(0.03, 0.07),  # Slightly broader focus on learning rates
    "model__n_estimators": randint(50, 150),  # Fewer boosting rounds
    "model__max_depth": randint(3, 7),  # Limit depth for better generalization
    "model__subsample": uniform(0.7, 0.3),  # Subsampling to prevent overfitting
    "model__colsample_bytree": uniform(0.7, 0.3),  # Column sampling
}

# Use fewer iterations and splits
random_search = RandomizedSearchCV(
    estimator=pipeline,
    param_distributions=param_dist,
    n_iter=15,  # Fewer iterations
    scoring="roc_auc",  # Optimize for ROC AUC
    cv=3,  # Use 3-fold CV for efficiency
    verbose=2,  # Print progress
    random_state=42,  # Ensure reproducibility
    n_jobs=-1  # Use all available processors
)

# Perform hyperparameter tuning
random_search.fit(X_train, y_train)

# Display the best parameters and best ROC AUC score
print("Best Parameters:", random_search.best_params_)
print("Best Cross-Validation ROC AUC Score:", random_search.best_score_)

# Evaluate the tuned model on the test set
best_pipeline = random_search.best_estimator_  # Get the best pipeline
y_test_pred = best_pipeline.predict(X_test)
y_test_pred_proba = best_pipeline.predict_proba(X_test)[:, 1]

# Calculate metrics for the test set
test_metrics = {
    "Recall": recall_score(y_test, y_test_pred),
    "Accuracy": accuracy_score(y_test, y_test_pred),
    "Precision": precision_score(y_test, y_test_pred),
    "F1 Score": f1_score(y_test, y_test_pred),
    "ROC AUC": roc_auc_score(y_test, y_test_pred_proba),
}

# Display the test set metrics
print("\nTest Set Performance with Tuned Model:")
print(pd.DataFrame(test_metrics, index=["Score"]).T)

# Evaluate the tuned model on the test set
best_pipeline = random_search.best_estimator_  # Get the best pipeline
y_train_pred = best_pipeline.predict(X_train)
y_train_pred_proba = best_pipeline.predict_proba(X_train)[:, 1]

# Calculate metrics for the test set
test_metrics = {
    "Recall": recall_score(y_train, y_train_pred),
    "Accuracy": accuracy_score(y_train, y_train_pred),
    "Precision": precision_score(y_train, y_train_pred),
    "F1 Score": f1_score(y_train, y_train_pred),
    "ROC AUC": roc_auc_score(y_train, y_train_pred_proba),
}

# Display the test set metrics
print("\nTrain Set Performance with Tuned Model:")
print(pd.DataFrame(test_metrics, index=["Score"]).T)

# Generate predicted probabilities for the positive class (1)
y_pred_curve = pipeline.predict_proba(X_test)[:, 1]  # Use the best pipeline from tuning

# Compute precision, recall, and thresholds
precision, recall, threshold = precision_recall_curve(y_test, y_pred_curve)

# Plot the Precision-Recall curve
prd = PrecisionRecallDisplay(precision, recall)
prd.plot()
plt.title("Precision-Recall Curve for LightGBM Model")
plt.show()

# Generate predicted probabilities for the positive class (1)
y_pred_curve = pipeline.predict_proba(X_test)[:, 1]  # Use the best pipeline from tuning

# Compute precision, recall, and thresholds
precision, recall, threshold = precision_recall_curve(y_test, y_pred_curve)

pr_table = pd.DataFrame({
    "Threshold": threshold,
    "Precision": precision[:-1],  # Last value of precision is omitted
    "Recall": recall[:-1],        # Last value of recall is omitted
})

# Find the threshold where precision is close to recall
pr_table["Difference"] = abs(pr_table["Precision"] - pr_table["Recall"])
optimal_threshold = pr_table.loc[pr_table["Difference"].idxmin(), "Threshold"]
print("Optimal Threshold:", optimal_threshold)
print("Optimal Recall:", pr_table.loc[pr_table["Difference"].idxmin(), ["Recall"]])
print("Optimal Precision:", pr_table.loc[pr_table["Difference"].idxmin(), ["Precision"]])

plt.figure(figsize=(10, 6))
plt.plot(threshold, precision[:-1], label="Precision", color="blue")
plt.plot(threshold, recall[:-1], label="Recall", color="orange")
plt.xlabel("Threshold", fontsize=14)
plt.ylabel("Score", fontsize=14)
plt.title("Precision-Recall vs. Threshold", fontsize=16)
plt.legend(loc="lower left", fontsize=12)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# Train the pipeline if not already fitted
pipeline.fit(X_train, y_train)

# Extract the trained model from the pipeline
model = pipeline.named_steps['model']

# Ensure the model provides feature importances (tree-based models like LightGBM)
if hasattr(model, 'feature_importances_'):
    # Extract feature importances
    feature_importances = model.feature_importances_

    # Get the numerical feature names
    numerical_features = preprocessor.transformers_[0][2]

    # Get the categorical feature names from the one-hot encoder
    categorical_features = pipeline.named_steps['preprocessor'].named_transformers_['cat']['onehot'].get_feature_names_out()

    # Combine numerical and categorical feature names
    all_features = list(numerical_features) + list(categorical_features)

    # Ensure the lengths match between feature names and importances
    if len(feature_importances) != len(all_features):
        print("Length mismatch between feature importances and feature names.")
    else:
        # Create a DataFrame for feature importance
        feature_importance_df = pd.DataFrame({'Feature': all_features, 'Importance': feature_importances})

        # Sort by importance for better visualization
        feature_importance_df = feature_importance_df.sort_values('Importance', ascending=True)

        # Plot feature importance
        plt.figure(figsize=(10, 20))
        feature_importance_df.plot(x='Feature', y='Importance', kind='barh', figsize=(10, 20), legend=False, color='skyblue')
        plt.title('Feature Importance')
        plt.xlabel('Importance')
        plt.ylabel('Feature')
        plt.tight_layout()
        plt.show()

else:
    print("The selected model does not provide feature importances.")

# Assuming the pipeline has been fitted on the training data
y_pred = pipeline.predict(X_test)  # Predict on the test set

# Generate confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot confusion matrix using seaborn heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

TP = cm[1][1]
FN = cm[1][0]
FP = cm[0][1]
TN = cm[0][0]

persentase_kesalahan_model = (FP / (TP+FP+TN+FN)) * 100

print(f'Persentase kesalahan prediksi resiko pinjaman yang menimbulkan kerugian: {persentase_kesalahan_model:.2f}%')

persentase_kesalahan_model = (FN / (TP+FP+TN+FN)) * 100

print(f'Persentase kesalahan prediksi resiko pinjaman yang menimbulkan profit tidak maksimal: {persentase_kesalahan_model:.2f}%')

import pickle

# Assume 'pipeline' is your pre-trained LightGBM pipeline object
# Save the pipeline to a pickle file
with open('lightgbm_pipeline.pkl', 'wb') as f:
    pickle.dump(pipeline, f)

print("Pipeline saved to 'lightgbm_pipeline.pkl'")